{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f75f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishal\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch import autograd\n",
    "from misc import *\n",
    "from PIL import ImageFile\n",
    "\n",
    "import os.path\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import random\n",
    "from torchvision.utils import save_image\n",
    "import cv2\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b88af9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = ['conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
    "               'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "               'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "               'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "               'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4', 'pool5']\n",
    "               \n",
    "content_layers = ['relu1_1', 'relu2_1', 'relu3_1']\n",
    "\n",
    "\n",
    "n_z = 50\n",
    "n_l = 5\n",
    "n_channel = 3\n",
    "n_disc = 16\n",
    "n_gen = 64\n",
    "nef = 64\n",
    "ndf = 64\n",
    "ngpu = 1\n",
    "n_z = 50\n",
    "n_l = 5\n",
    "n_channel = 3\n",
    "n_disc = 16\n",
    "n_gen = 64\n",
    "n_age = int(n_z/n_l) #12\n",
    "n_gender = int(n_z/2) #25\n",
    "image_size = 128\n",
    "nz = int(n_z)\n",
    "nef = int(nef)\n",
    "ndf = int(ndf)\n",
    "nc = 3\n",
    "out_size = image_size // 16  # 64\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84b96695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attection Block\n",
    "class Self_Attn(nn.Module):\n",
    "    def __init__(self,in_dim,activation):\n",
    "        super(Self_Attn,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.action = activation\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim,out_channels=in_dim//8,kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_dim,out_channels=in_dim//8,kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim,out_channels=in_dim,kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax  = nn.Softmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        m_batchsize,C,width,height = x.size()\n",
    "        \n",
    "        proj_query = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize,-1,width*height)\n",
    "        energy = torch.bmm(proj_query,proj_key) #batch matrix-matrix product of matrices store\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value  = self.value_conv(x).view(m_batchsize,-1,width*height)\n",
    "        out  = torch.bmm(proj_value,attention.permute(0,2,1))\n",
    "        out  = out.view(m_batchsize,C,width,height)\n",
    "        out = self.gamma*out +x\n",
    "        return out,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7397a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residule Block\n",
    "class resnet_block(nn.Module):\n",
    "    def __init__(self, channel, kernel, stride, padding):\n",
    "        super(resnet_block, self).__init__()\n",
    "        self.channel = channel\n",
    "        self.kernel = kernel\n",
    "        self.strdie = stride\n",
    "        self.padding = padding\n",
    "        self.conv1 = nn.Conv2d(channel, channel, kernel, stride, padding)\n",
    "        self.conv1_norm = nn.BatchNorm2d(channel)\n",
    "        self.conv2 = nn.Conv2d(channel, channel, kernel, stride, padding)\n",
    "        self.conv2_norm = nn.BatchNorm2d(channel)\n",
    "        #self.initialize_weights()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.conv1_norm(self.conv1(input)), True)\n",
    "        x = self.conv2_norm(self.conv2(x))\n",
    "\n",
    "        return input + x  # Elementwise Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf292479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(nc, nef, 4, 2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(nef, nef * 2, 4, 2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.encoder_second = nn.Sequential(\n",
    "            nn.Conv2d(nef * 2, nef * 4, 4, 2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(nef * 4, nef * 8, 4, 2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.resnet_blocks = []\n",
    "        for i in range(9):\n",
    "            self.resnet_blocks.append(resnet_block(nef * 2, 3, 1, 1))\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(*self.resnet_blocks)\n",
    "        self.attn1  = Self_Attn(512,'relu')\n",
    "        self.mean = nn.Linear(nef * 8 * out_size * out_size, nz)\n",
    "        self.logvar = nn.Linear(nef * 8 * out_size * out_size, nz)\n",
    "    \n",
    "    def sampler(self, mean, logvar):\n",
    "        \n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mean)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        \n",
    "        hidden = self.encoder(input)\n",
    "        hidden = self.resnet_blocks(hidden)\n",
    "        hidden = self.encoder_second(hidden)\n",
    "        \n",
    "        out,ep1  = self.attn1(hidden)\n",
    "        \n",
    "        hidden = out.view(batch_size, -1)\n",
    "        mean, logvar = self.mean(hidden), self.logvar(hidden)\n",
    "        latent_z = self.sampler(mean, logvar)\n",
    "        return latent_z,ep1\n",
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87dae68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    " \n",
    "        self.decoder_dense = nn.Sequential(\n",
    "            nn.Linear(n_z+n_l*n_age+n_gender, ndf * 8 * out_size * out_size),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            nn.Conv2d(ndf * 8, ndf * 4, 3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            nn.Conv2d(ndf * 4, ndf * 2, 3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            nn.Conv2d(ndf * 2, ndf, 3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            nn.Conv2d(ndf, nc, 3, padding=1),\n",
    "            nn.Tanh()\n",
    "\n",
    "        )\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, z, age, gender):\n",
    "        batch_size = z.size(0)\n",
    "        n_age = age.size(1)  # Update to the correct size of age\n",
    "        n_gender = gender.size(1)  # Update to the correct size of gender\n",
    "    \n",
    "        l = age.repeat(1, n_age)  # Repeat age correctly\n",
    "        k = gender.repeat(1, n_gender)  # Repeat gender correctly\n",
    "    \n",
    "        x = torch.cat([z, l, k.float()], dim=1)  # Concatenate tensors\n",
    "        hidden = self.decoder_dense(x)  # No need to reshape here\n",
    "        output = self.decoder_conv(hidden.view(batch_size, ndf * 8, out_size, out_size))  # Reshape before passing to convolutional layers\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "babbba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a23a60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train encoder and decoder model weights    \n",
    "c = torch.load(r\"C:\\Users\\vishal\\Desktop\\Faceaging new 3\\Path of Output\\encoder_epoch_0.pth\", map_location=torch.device('cpu'))\n",
    "encoder.load_state_dict(c)\n",
    "d = torch.load(r\"C:\\Users\\vishal\\Desktop\\Faceaging new 3\\Path of Output\\decoder_epoch_0.pth\", map_location=torch.device('cpu'))\n",
    "decoder.load_state_dict(d)\n",
    "\n",
    "outf=\"Result/\"\n",
    "if not os.path.exists(outf):\n",
    "    os.mkdir(outf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88afedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(img_dir, label, batch_size=2, img_size=128, mode=\"train\", num_workers=1):\n",
    "    transform = []\n",
    "    transform.append(transforms.Resize(img_size))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    \n",
    "    # Adjust normalization based on the number of channels\n",
    "    if mode == \"grayscale\":\n",
    "        # Grayscale images have a single channel\n",
    "        transform.append(transforms.Normalize(mean=[0.5], std=[0.5]))\n",
    "    else:\n",
    "        # RGB images have three channels\n",
    "        transform.append(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]))\n",
    "    \n",
    "    transform = transforms.Compose(transform)\n",
    "    file_list = []\n",
    "    \n",
    "    for dir in os.listdir(img_dir):\n",
    "        print(dir)\n",
    "        path, dirs, files = next(os.walk((os.path.join(img_dir, dir))))\n",
    "        print(\"files\", files)\n",
    "        \n",
    "        if len(files) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            for file in os.listdir(os.path.join(img_dir, dir)):\n",
    "                file_list.append(os.path.join(img_dir, dir) + \"/\" + file)\n",
    "\n",
    "    filenames = []\n",
    "    unchanged_target_ages = []\n",
    "    images = []\n",
    "    targets = []\n",
    "    target_genders = np.ones((len(file_list), 1), dtype=np.int32) * -1\n",
    "    \n",
    "    for line in file_list:\n",
    "        image = Image.open(line)\n",
    "        image = transform(image)\n",
    "        images.append(np.array(image))\n",
    "    \n",
    "    images = np.array(images)\n",
    "    return images, file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87ee149a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "files ['1_0_0_20161219140623097.jpg.chip.jpg', '1_0_0_20161219140627985.jpg.chip.jpg', '1_0_0_20161219140642920.jpg.chip.jpg', '1_0_0_20161219154018476.jpg.chip.jpg', '1_0_0_20161219154556757.jpg.chip.jpg', '1_0_0_20161219154724341.jpg.chip.jpg', '1_0_0_20161219154909149.jpg.chip.jpg', '1_0_0_20161219154956869.jpg.chip.jpg', '1_0_0_20161219160713534.jpg.chip.jpg']\n",
      "Test\n",
      "files ['1_0_0_20161219140623097.jpg.chip.jpg', '1_0_0_20161219140627985.jpg.chip.jpg', '1_0_0_20161219140642920.jpg.chip.jpg', '1_0_0_20161219154018476.jpg.chip.jpg', '1_0_0_20161219154556757.jpg.chip.jpg', '1_0_0_20161219154724341.jpg.chip.jpg', '1_0_0_20161219154909149.jpg.chip.jpg', '1_0_0_20161219154956869.jpg.chip.jpg', '1_0_0_20161219160713534.jpg.chip.jpg']\n"
     ]
    }
   ],
   "source": [
    "# For RGB images\n",
    "images, file_list = get_loader(img_dir=r\"C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\", img_size=128, label=0, batch_size=8)\n",
    "\n",
    "# For grayscale images\n",
    "images, file_list = get_loader(img_dir=r\"C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\", img_size=128, label=0, batch_size=8, mode=\"grayscale\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2548e405",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "files ['1_0_0_20161219140623097.jpg.chip.jpg', '1_0_0_20161219140627985.jpg.chip.jpg', '1_0_0_20161219140642920.jpg.chip.jpg', '1_0_0_20161219154018476.jpg.chip.jpg', '1_0_0_20161219154556757.jpg.chip.jpg', '1_0_0_20161219154724341.jpg.chip.jpg', '1_0_0_20161219154909149.jpg.chip.jpg', '1_0_0_20161219154956869.jpg.chip.jpg', '1_0_0_20161219160713534.jpg.chip.jpg']\n",
      "['C:\\\\Users\\\\vishal\\\\Desktop\\\\Faceaging new 3\\\\testing\\\\Test/1_0_0_20161219140623097.jpg.chip.jpg', 'C:\\\\Users\\\\vishal\\\\Desktop\\\\Faceaging new 3\\\\testing\\\\Test/1_0_0_20161219140627985.jpg.chip.jpg', 'C:\\\\Users\\\\vishal\\\\Desktop\\\\Faceaging new 3\\\\testing\\\\Test/1_0_0_20161219140642920.jpg.chip.jpg', 'C:\\\\Users\\\\vishal\\\\Desktop\\\\Faceaging new 3\\\\testing\\\\Test/1_0_0_20161219154018476.jpg.chip.jpg', 'C:\\\\Users\\\\vishal\\\\Desktop\\\\Faceaging new 3\\\\testing\\\\Test/1_0_0_20161219154556757.jpg.chip.jpg', 'C:\\\\Users\\\\vishal\\\\Desktop\\\\Faceaging new 3\\\\testing\\\\Test/1_0_0_20161219154724341.jpg.chip.jpg', 'C:\\\\Users\\\\vishal\\\\Desktop\\\\Faceaging new 3\\\\testing\\\\Test/1_0_0_20161219154909149.jpg.chip.jpg', 'C:\\\\Users\\\\vishal\\\\Desktop\\\\Faceaging new 3\\\\testing\\\\Test/1_0_0_20161219154956869.jpg.chip.jpg', 'C:\\\\Users\\\\vishal\\\\Desktop\\\\Faceaging new 3\\\\testing\\\\Test/1_0_0_20161219160713534.jpg.chip.jpg']\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219140623097.jpg.chip.jpg\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219140623097.jpg.chip.jpg : 0\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219140627985.jpg.chip.jpg\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219140627985.jpg.chip.jpg : 0\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219140642920.jpg.chip.jpg\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219140642920.jpg.chip.jpg : 0\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219154018476.jpg.chip.jpg\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219154018476.jpg.chip.jpg : 0\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219154556757.jpg.chip.jpg\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219154556757.jpg.chip.jpg : 0\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219154724341.jpg.chip.jpg\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219154724341.jpg.chip.jpg : 0\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219154909149.jpg.chip.jpg\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219154909149.jpg.chip.jpg : 0\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219154956869.jpg.chip.jpg\n",
      "C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\\Test/1_0_0_20161219154956869.jpg.chip.jpg : 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (40x76 and 125x32768)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m fixed_g_v \u001b[38;5;241m=\u001b[39m Variable(fixed_g)\n\u001b[0;32m     30\u001b[0m fixed_l_v \u001b[38;5;241m=\u001b[39m Variable(fixed_l)\n\u001b[1;32m---> 31\u001b[0m fixed_fake \u001b[38;5;241m=\u001b[39m decoder(fixed_z,fixed_l_v,fixed_g_v)\n\u001b[0;32m     32\u001b[0m outputpath \u001b[38;5;241m=\u001b[39m outf\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(batch)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m vutils\u001b[38;5;241m.\u001b[39msave_image(fixed_fake\u001b[38;5;241m.\u001b[39mdata,outputpath , normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 37\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, z, age, gender)\u001b[0m\n\u001b[0;32m     34\u001b[0m k \u001b[38;5;241m=\u001b[39m gender\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, n_gender)  \u001b[38;5;66;03m# Repeat gender correctly\u001b[39;00m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([z, l, k\u001b[38;5;241m.\u001b[39mfloat()], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Concatenate tensors\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_dense(x)  \u001b[38;5;66;03m# No need to reshape here\u001b[39;00m\n\u001b[0;32m     38\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_conv(hidden\u001b[38;5;241m.\u001b[39mview(batch_size, ndf \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m, out_size, out_size))  \u001b[38;5;66;03m# Reshape before passing to convolutional layers\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (40x76 and 125x32768)"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Test is directory\n",
    "images,file_list=  get_loader(img_dir=r\"C:\\Users\\vishal\\Desktop\\Faceaging new 3\\testing\",img_size=128,label=0,batch_size=8)\n",
    "print(file_list)\n",
    "images = torch.FloatTensor(images)\n",
    "batch_size = 8\n",
    "import math\n",
    "import scipy.misc\n",
    "num_batches = int(math.ceil(images.shape[0]/batch_size))\n",
    "i=0\n",
    "for batch in range(num_batches):\n",
    "    batch_image = images[batch*batch_size:(batch+1)*batch_size,:,:,:].repeat(5, 1, 1, 1)\n",
    "    file_batch = file_list[batch*batch_size:(batch+1)*batch_size]\n",
    "    fixed_l = -torch.ones(40 * 5).view(40, 5)\n",
    "    for i, l in enumerate(fixed_l):\n",
    "        l[i // 8] = 1\n",
    "    fixed_z,ep1 = encoder(batch_image)\n",
    "    target_genders = -torch.ones(batch_size*1).view(batch_size,1)\n",
    "    fcount=0\n",
    "    for file_name  in file_batch:\n",
    "        print(file_name)\n",
    "        \n",
    "        target_gender = int(file_name.split(\"/\")[-1].split(\"_\")[2])\n",
    "        print(file_name,\":\",target_gender)\n",
    "        \n",
    "        if (target_gender == 1):\n",
    "            target_genders[fcount] = 1\n",
    "        fcount=fcount+1\n",
    "    fixed_g= target_genders.view(-1, 1).repeat(5, 1)\n",
    "    fixed_g_v = Variable(fixed_g)\n",
    "    fixed_l_v = Variable(fixed_l)\n",
    "    fixed_fake = decoder(fixed_z,fixed_l_v,fixed_g_v)\n",
    "    outputpath = outf+\"/\"+str(batch)+\".jpg\"\n",
    "    vutils.save_image(fixed_fake.data,outputpath , normalize=True)\n",
    "    print(\"output path\",outputpath)\n",
    "    img = Image.open(outputpath)\n",
    "    noOfRow = 5\n",
    "    noOfColumn = 8\n",
    "    x1 = 2\n",
    "    y1 = 2\n",
    "    x2 = 130\n",
    "    y2 = 130\n",
    "    folder = file_batch\n",
    "    # Store result according the test image id.\n",
    "    for i in range(0, noOfColumn):\n",
    "        dest_dir = file_batch[i].split(\"/\")[-1]\n",
    "        if not os.path.exists(outf+\"/\"+dest_dir):\n",
    "            os.mkdir(outf+\"/\"+dest_dir)\n",
    "        for j in range(1, noOfRow + 1):\n",
    "            area = (x1, y1, x2, y2)\n",
    "            cropped_img = img.crop(area)\n",
    "            imgName = \"{}{}\".format(i, j)\n",
    "            if(int(imgName)==1 or int(imgName)==11 or int(imgName)==21 or int(imgName)==31 or int(imgName)==41 or int(imgName)==51 or int(imgName)==61 or int(imgName)==71):\n",
    "                filename= \"cat1_\"+file_batch[i].split(\"/\")[-1]\n",
    "                shutil.copy(file_batch[i],os.path.join(outf+dest_dir,file_batch[i].split(\"/\")[-1]))\n",
    "                \n",
    "            if (int(imgName) == 2 or int(imgName) == 12 or int(imgName) == 22 or int(imgName) == 32 or int(imgName) == 42 or int(imgName) == 52 or int(imgName) == 62 or int(imgName) == 72):\n",
    "                filename = \"cat2_\" + file_batch[i].split(\"/\")[-1]\n",
    "            if (int(imgName) == 3 or int(imgName) == 13 or int(imgName) == 23 or int(imgName) == 33 or int(imgName) == 43 or int(imgName) == 53 or int(imgName) == 63 or int(imgName) == 73):\n",
    "                filename = \"cat3_\" + file_batch[i].split(\"/\")[-1]\n",
    "            if (int(imgName) == 4 or int(imgName) == 14 or int(imgName) == 24 or int(imgName) == 34 or int(imgName) == 44 or int(imgName) == 54 or int(imgName) == 64 or int(imgName) == 74):\n",
    "                filename = \"cat4_\" + file_batch[i].split(\"/\")[-1]\n",
    "            if (int(imgName) == 5 or int(imgName) == 15 or int(imgName) == 25 or int(imgName) == 35 or int(imgName) == 45 or int(imgName) == 55 or int(imgName) == 65 or int(imgName) == 75):\n",
    "                filename = \"cat5_\" + file_batch[i].split(\"/\")[-1]\n",
    "            print(file_batch[i],os.path.join(outf+dest_dir,filename))\n",
    "            cropped_img.save(os.path.join(outf+dest_dir,filename))\n",
    "            y1 = y1 + 130\n",
    "            y2 = y2 + 130\n",
    "        x1 = x1 + 130\n",
    "        x2 = x2 + 130\n",
    "        y1 = 2\n",
    "        y2 = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161425c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ddb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a22a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15adcae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_loader(img_dir,label,batch_size=2,img_size=128,mode=\"train\",num_workers=1):\n",
    "    transform=[]\n",
    "    transform.append(transforms.Resize(img_size))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
    "    transform = transforms.Compose(transform)\n",
    "    file_list=[]\n",
    "    for dir in os.listdir(img_dir):\n",
    "        print(dir)\n",
    "        path, dirs, files = next(os.walk((os.path.join(img_dir, dir))))\n",
    "        print(\"files\",files)\n",
    "        if (len(files) == 1):\n",
    "            pass\n",
    "        else:\n",
    "            for file in os.listdir(os.path.join(img_dir, dir)):\n",
    "                file_list.append(os.path.join(img_dir, dir)+\"/\"+file)\n",
    "\n",
    "    filenames = []\n",
    "    unchanged_target_ages = []\n",
    "    images = []\n",
    "    targets = []\n",
    "    target_genders = np.ones((len(file_list), 1), dtype=np.int32) * -1\n",
    "    for line in file_list:\n",
    "\n",
    "        image = Image.open(line)\n",
    "        image = transform(image)\n",
    "        images.append(np.array(image))\n",
    "    images = np.array(images)\n",
    "    return  images,file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0bb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, z,age,gender):\n",
    "        batch_size = z.size(0)\n",
    "        l = age.repeat(1, n_age)  # size = 20 * 48\n",
    "        k = gender.view(-1, 1).repeat(1, n_gender)  # size = 20 * 25\n",
    "        x = torch.cat([z, l, k.float()], dim=1)  # size = 20 * 123\n",
    "        hidden = self.decoder_dense(x).view(batch_size,ndf * 8, out_size, out_size)\n",
    "        output = self.decoder_conv(hidden)\n",
    "        return output\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
